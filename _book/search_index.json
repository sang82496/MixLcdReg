[["index.html", "Creating the mixLcdReg R package 1 Preliminaries 1.1 DESCRIPTION file 1.2 Package-level documentation", " Creating the mixLcdReg R package Sang-wook Lee 2023-12-20 1 Preliminaries This dom cument uses litr to define the mixLcdReg R package. When the index.Rmd file is rendered, the R package is created along with the bookdown you are reading. To do so in RStudio, you can simply open index.Rmd and press “Knit” to render the bookdown (and open _book/index.html to see the result). More generally, in a console you can run the following: litr::render(&quot;index.Rmd&quot;) 1.1 DESCRIPTION file We start by specifying some basic information for the description file: usethis::create_package( path = &quot;.&quot;, fields = list( Package = params$package_name, Version = &quot;0.0.0.9000&quot;, Title = &quot;Mixture of log-concave regressions&quot;, Description = &quot;This package uses mixture of log-concave regressions to analyze complex and heterogenous data.&quot;, `Authors@R` = person( given = &quot;Sang-wook&quot;, family = &quot;Lee&quot;, email = &quot;sangwook@usc.edu&quot;, role = c(&quot;aut&quot;, &quot;cre&quot;) ) ) ) usethis::use_mit_license(copyright_holder = &quot;F. Last&quot;) 1.2 Package-level documentation Let’s include some package-level documentation. Besides being user-friendly, it’s also needed because we’ll be using “import from” later. #&#39; Mixture of log-concave regressions modeling #&#39; #&#39; This package uses mixture of log-concave regressions to analyze complex and heterogenous data. #&#39; #&#39; @docType package "],["the-model.html", "2 The model 2.1 Generating data from model 2.2 Visualizing data in one-dimensional case 2.3 Visualizing data and model in one-dimensional case", " 2 The model Let \\(X^{(t)} \\in \\mathbb{R}^{p}\\) be given fixed design points and \\(Y_i^{(t)} \\in \\mathbb{R}^{d}\\) be the observed dependent variables, for \\(1 \\le i \\le n_t, \\ 1 \\le t \\le T\\). We model \\(Y_i^{(t)}\\) as i.i.d. draws from \\(K\\) different \\(d-\\)dimensional log-concave distributions, conditioning on \\(X^{(t)}\\). Let \\(Z_i^{(t)}\\) denote the (latent) cluster membership so that \\(P(Z_i^{(t)} =k|X^{(t)} ) = \\pi_k(X^{(t)} )\\), for \\(1 \\le k \\le K\\). Let \\(\\mu_k:\\mathbb{R}^{p+1} \\rightarrow \\mathbb{R}^{d}\\) be the unknown regression function, for \\(1 \\le k \\le K\\) and \\(\\mu_k\\) is assumed to belong to a given family \\(M\\), which is closed under scalar addition. We further assume that \\(M\\) is the set of all affine functions so that \\(\\mu_k \\in M\\) if and only if \\(\\mu_k (x) = \\theta_{k0} + \\theta_k^T x\\) for some coefficients \\(\\theta_{k0} \\in \\mathbb{R}^{d}\\) and \\(\\theta_k \\in \\mathbb{R}^{p \\times d}\\). The conditional error \\(\\varepsilon_i^{(t)} |Z_i^{(t)} =k\\) follows a mean zero, \\(d-\\)dimensional log-concave density \\(exp(g_k)\\) so that \\(Y_i^{(t)} = \\theta_{k0} + \\theta_k^T X^{(t)} + \\varepsilon_i^{(t)}, \\text{ if } Z_i^{(t)} =k\\). Then, \\[ Y_i^{(t)} |Z_i^{(t)} = k, X^{(t)} \\ \\sim \\ exp[g_k(\\cdot - \\theta_{k0} - \\theta_k^T X^{(t)} )] \\] so that \\[ Y_i^{(t)} | X^{(t)} \\ \\sim \\ \\sum_{k=1}^K exp[g_k(\\cdot - \\theta_{k0} - \\theta_k^T X^{(t)} )]\\pi_k(X^{(t)} ) \\] We model \\[ \\pi_{tk}(\\alpha ) = \\pi_k(X^{(t)} ; \\alpha) = \\frac{exp(\\alpha_{k0} + \\alpha_k^T X^{(t)} )}{\\sum_{l=1}^K exp(\\alpha_{l0} + \\alpha_l^T X^{(t)} )} \\] where \\(\\alpha = \\{ \\alpha_{k0}, \\alpha_{k} \\}_{k=1}^K\\) is a collection of coefficients \\(\\alpha_{k0} \\in \\mathbb{R}\\) and \\(\\alpha_k \\in \\mathbb{R}^{p}\\) for \\(1 \\le k \\le K\\). What we want to maximize here is the mixture of log-likelihood: \\[\\begin{align*} L(\\alpha, \\theta, g; X, Y) &amp;= \\frac{1}{N} \\sum_{t=1}^T \\sum_{i=1}^{n_t} \\log \\mathbb{P}(Y_i^{(t)} |X^{(t)} ) \\\\ &amp;= \\frac{1}{N} \\sum_{t=1}^T \\sum_{i=1}^{n_t} \\log \\left[ \\sum_{k=1}^K exp \\left( g_k \\left(Y_i^{(t)} -\\theta_{k0} - \\theta_k^T X^{(t)} \\right) \\right) \\cdot \\pi_{tk}(\\alpha) \\right] \\end{align*}\\] where the parameters are \\(\\alpha,\\ \\theta = \\{ \\theta_{k0},\\theta_k \\}_{k=1}^K,\\ g = \\{ g_k \\}_{k=1}^K\\) where \\(g_k\\) is a logdensity of a logconcave density \\(exp(g_k)\\) for \\(1 \\le k \\le K\\), and \\(X = \\{X^{(t)} \\}_{t=1}^T,\\ Y = \\{ Y_i^{(t)} \\}_{i = 1,..., n_t}^{t = 1,..., T},\\ N = \\sum_{t=1}^T n_t\\) But directly optimizing \\(L(\\alpha, \\theta, g; X, Y)\\) is difficult due to its non-convexity. Instead, using the membership \\(Z_i^{(t)}\\), we define the joint log-likelihood: \\[\\begin{align*} \\Lambda(\\alpha, \\theta, g; X, Y, Z) &amp;= \\frac{1}{N} \\sum_{t=1}^T \\sum_{i=1}^{n_t} \\sum_{k=1}^K \\mathbb{I}\\{Z_i^{(t)} =k\\} \\log \\mathbb{P}(Y_i^{(t)} , Z_i^{(t)} = k |X^{(t)} ) \\\\ &amp;= \\frac{1}{N} \\sum_{t, i,k} \\mathbb{I}\\{Z_i^{(t)} =k\\} \\left[ g_k \\left(Y_i^{(t)} -\\theta_{k0} -\\theta_k^T X^{(t)} \\right) + \\log \\pi_{tk}(\\alpha) \\right] \\end{align*}\\] Since we can’t observe \\(Z_i^{(t)}\\), we define the surrogate function, which is the conditional expectation of \\(\\Lambda(\\alpha, g, \\theta; X, Y, Z)\\) with respect to the membership \\(Z_i^{(t)}\\), conditioning on \\(X^{(t)}\\) and \\(Y_i^{(t)}\\): \\[ Q(\\alpha, g, \\theta) = Q(\\alpha, g, \\theta; X, Y) = \\frac{1}{N} \\sum_{t=1}^T \\sum_{i=1}^{n_t} \\sum_{k=1}^K r_{tik} \\left[ g_k \\left(Y_i^{(t)} -\\theta_{k0} -\\theta_k^T X^{(t)} \\right)) + \\log \\pi_{tk}(\\alpha) \\right] \\] where the responsibility \\(r_{tik}\\) is defined \\[\\begin{align*} r_{tik} = r_{tik}(\\alpha, \\theta, g) = P(Z_i^{(t)} = k |Y_i^{(t)} , X^{(t)} ; \\alpha, \\theta, g) &amp;= \\frac{exp(g_k(Y_i^{(t)} -\\theta_{k0} -\\theta_k^T X^{(t)} )) \\pi_{tk}(\\alpha)} {\\sum_{l=1}^K exp(g_l(Y_i^{(t)} -\\theta_{l0} -\\theta_l^T X^{(t)} )) \\pi_{tl}(\\alpha) } \\end{align*}\\] Note that the responsibility is a probability, so it should sum up to one with respect to k in the sense that \\[ \\sum_{k=1}^K r_{tik} = \\sum_{k=1}^K P(Z_i^{(t)} = k |Y_i^{(t)} , X^{(t)} ) = 1 \\] It will be useful to have data generated from this model for testing purposes, so we begin by defining a function for simulating from this model. 2.1 Generating data from model The function will take as input the following arguments: ###&quot;theta0-param&quot;### #&#39; @param theta0 length K list, with `theta0[[k]]` being the estimate for the intercept coefficient #&#39; of the regression for kth group ###&quot;theta-param&quot;### #&#39; @param theta length K list, with `theta[[k]]` being the p-by-1 vector. Initial estimate for coefficients #&#39; of the regression for kth group ###&quot;alpha-param&quot;### #&#39; @param alpha (p+1)-by-K matrix. The coefficients for the cluster probabilities. ###&quot;X-param&quot;### #&#39; @param X a T-by-p matrix of covariates, where `X[[t]]` being the p-vector of independent variable at time t We define this bit of documentation in its own code chunk so that it can be easily reused since multiple functions in the package take it as input. #&#39; Generate data from mixture of log-concave regressions #&#39; #&#39; The mixture of log-concave regressions model is defined as follows: #&#39; #&#39; At time t there are n_t points generated as follows: #&#39; #&#39; Given Z_i^{(t)} = k, Y_i^{(t)} = theta_{k0} + theta_k^T X^{(t)} + epsilon_i^{(t)} #&#39; #&#39; where epsilon_i^{(t)} ~ exp(g_k) #&#39; #&#39; and P(Z_i^{(t)} =k |X^{(t)} ;alpha) = \\frac{exp(alpha_{k0} + alpha_k^T X^{(t)} )}{\\sum_{l=1}^K exp(alpha_{l0} + alpha_l^T X^{(t)} )} #&#39; #&#39; This function generates Y and Z #&#39; #&#39; For simplicity we here assume alpha = 0, K = 2, p = 2 and d = 1 #&#39; &lt;&lt;X-param&gt;&gt; &lt;&lt;theta0-param&gt;&gt; &lt;&lt;theta-param&gt;&gt; &lt;&lt;alpha-param&gt;&gt; #&#39; @param num_points T-vector of integers giving the number of points n_t to #&#39; generate at each time point t. #&#39; @param err_type a K-vector of strings, each of which is the type of error distribution #&#39; for the each group. Now, it is one of &#39;Gaussian&#39;, &#39;exp&#39; #&#39; @return Y length T list with `Y[[t]]` being a n_t-by-d matrix #&#39; @return Z length T list with `Z[[t]]` being a n_t vector of cluster memberships #&#39; @export generate_mix_lcd_reg &lt;- function(X, theta0, theta, alpha, num_points, err_type = c(&#39;Gaussian&#39;, &#39;exp&#39;)) { TT = dim(X)[1] p = dim(X)[2] Y = list() Z = list() for (t in 1:TT){ tmp = exp(alpha[1,] + X[t,] %*% alpha[2:(p+1),]) prob = tmp / sum(tmp) Z[[t]] = rbinom(num_points[t], 1, prob[2]) n1 = sum(Z[[t]] == 0) n2 = sum(Z[[t]] == 1) Xt1 = theta0[[1]] + X[t,] %*% as.matrix(theta[[1]]) Xt2 = theta0[[2]] + X[t,] %*% as.matrix(theta[[2]]) if (err_type[1] == &#39;Gaussian&#39;) { e1 = rnorm(n1) } else { e1 = rexp(n1, 1) - 1 } if (err_type[2] == &#39;Gaussian&#39;) { e2 = rnorm(n2) } else { e2 = rexp(n2, 1) - 1 } Y[[t]] = rep(NA, num_points[t]) Y[[t]][Z[[t]] == 0] = rep(Xt1, n1) + e1 Y[[t]][Z[[t]] == 1] = rep(Xt2, n2) + e2 } return(list(Y = Y, Z = Z)) } Let’s generate an example in the \\(d=1\\) case: set.seed(1) d = 1; K = 2; TT = 50 X1 = sin((1:TT)*pi/12) X2 = c(rep(0, TT/2), rep(1, TT/2)) X = cbind(X1, X2) theta_true = list(c(0, 0), c(1/2, 0)) theta0_true = list(0, 5) num_points = rep(50, TT) alpha_true = matrix(rep(0, K*3), ncol = K) alpha_true[3,2] = 1 dat = generate_mix_lcd_reg(X, theta0_true, theta_true, alpha_true, num_points, c(&#39;Gaussian&#39;, &#39;exp&#39;)) Y = dat$Y Z = dat$Z 2.2 Visualizing data in one-dimensional case Let’s make a function for visualizing the data in the one-dimensional case. ###&quot;Y-param&quot;### #&#39; @param Y length T list with `Y[[t]]` being a n_t-by-d matrix #&#39; Plot raw data when `d = 1` #&#39; &lt;&lt;Y-param&gt;&gt; #&#39; @export plot_data &lt;- function(Y) { par(mfrow = c(1, 2), pty = &#39;s&#39;, cex = 0.7) plot(c(0, 0), xlim = c(1, TT), ylim = range(unlist(Y)), col = &#39;white&#39;) for (t in 1:TT){ points(rep(t, length(Y[[t]])), Y[[t]]) } } Let’s look at our example data using this plotting function: plot_data(Y) 2.3 Visualizing data and model in one-dimensional case We’ll also want a function for plotting the data with points colored by true (or estimated) cluster. And it will be convenient to also be able to superimpose the true (or estimated) means. The next function does this: #&#39; Plot data colored by cluster assignment with cluster means when `d=1` #&#39; &lt;&lt;X-param&gt;&gt; &lt;&lt;Y-param&gt;&gt; #&#39; @param Z a length T list with `Z[[t]]` being a n_t vector of cluster assignments &lt;&lt;theta0-param&gt;&gt; &lt;&lt;theta-param&gt;&gt; #&#39; @export plot_data_and_model &lt;- function(X, Y, Z, theta0, theta) { plot(c(0, 0), xlim = c(1, TT), ylim = c(-2, 10), col = &#39;white&#39;) for (t in 1:TT){ points(rep(t, sum(Z[[t]]==0)), Y[[t]][Z[[t]]==0], col = &#39;red&#39;) points(rep(t, sum(Z[[t]]==1)), Y[[t]][Z[[t]]==1], col = &#39;blue&#39;) } lines(1:TT, rep(theta0[[1]], TT) + X %*% theta[[1]], col = &#39;black&#39;, lwd = 2) lines(1:TT, rep(theta0[[2]], TT) + X %*% theta[[2]], col = &#39;black&#39;, lwd = 2) } For now we can use this to visualize the true model, although later this will be useful for visualizing the estimated model. par(mfrow = c(1, 2), pty = &#39;s&#39;, cex = 0.7) plot_data_and_model(X, Y, Z, theta0_true, theta_true) "],["the-method.html", "3 The method 3.1 preprocessing 3.2 Initialization 3.3 E-step 3.4 M-step 3.5 Trying out the method", " 3 The method We are using Expectation-Maximization(EM) algorithm to maximize the surrogate log-likelihood \\(Q(\\alpha, g, \\theta) = \\frac{1}{N} \\sum_{t=1}^T \\sum_{i=1}^{n_t} \\sum_{k=1}^K r_{tik} \\left[ g_k \\left(Y_i^{(t)} -\\theta_{k0} -\\theta_k^T X^{(t)} \\right)) + \\log \\pi_{tk}(\\alpha) \\right]\\). Here’s a high-level look at the algorithm. ###&quot;B-param&quot;### #&#39; @param number of binning ###&quot;K-param&quot;### #&#39; @param K number of clusters ###&quot;r_bar-param&quot;### #&#39; @param r_bar threshold for responsibility ###&quot;lambda_alpha-param&quot;### #&#39; @param lambda_alpha penalty parameter for alpha ###&quot;lambda_theta-param&quot;### #&#39; @param lambda_theta penalty parameter for theta #&#39; Mixture of log-concave regression #&#39; &lt;&lt;X-param&gt;&gt; &lt;&lt;Y-param&gt;&gt; &lt;&lt;K-param&gt;&gt; &lt;&lt;B-param&gt;&gt; #&#39; @param min_count_ratio &lt;&lt;r_bar-param&gt;&gt; &lt;&lt;lambda_alpha-param&gt;&gt; &lt;&lt;lambda_theta-param&gt;&gt; #&#39; @param max_iter number of maximum iterations of EM to perform #&#39; @param iter_eta threshold for the iteration. If the increment of loglikelihood is smaller than iter_eta, #&#39; we terminate the iterations. #&#39; # mixLcdReg &lt;- function(X, # Y, # K, # B = 40, # min_count_ratio = 0, # r_bar, # lambda_alpha, # lambda_theta, # max_iter = 100, # iter_eta = 1e-6) { # preprocessing # initialization with flexmix # iteration # for (i in seq(max_iter)) { ## E-step ## M-step ### M-step alpha ### M-step theta ### M-step shift ### M-step g ## termination criteria # } # return #} Here is more detailed explanation of the algorithm: 3.1 preprocessing Before initializing the parameters, we need to apply binning to \\(Y\\). Since it is better to deal with a matrix or a vector than a list, we are resizing Y. Resized Y has \\(N_{bin} \\times d\\) dimension, where \\(N_{B_t}\\) is the number of non-empty bins in time \\(t\\) and \\(N_{bin} = \\sum_{t = 1}^T N_{B_t}\\). Since \\(N_{B_t} \\le B\\), \\(N_{bin} \\le TB\\). Also, to make \\(X\\) have same row-dimension of \\(Y\\), we need to repeat \\(X^{(t)}\\), which is \\(t^{th}\\) row of \\(X\\), \\(N_{B_t}\\) times. Note that T means True in R, we use TT instead of T for time so that \\(t = 1,..., TT\\) #&#39; Binning on Y #&#39; &lt;&lt;X-param&gt;&gt; &lt;&lt;Y-param&gt;&gt; &lt;&lt;B-param&gt;&gt; #&#39; @param min_count_ratio #&#39; @return Y_bin N_bin-by-d matrix, indicating the center of the bins #&#39; @return Y_count N_bin-vector indicating how many points belong to each bin #&#39; @return X_bin N_bin-by-p matrix, which is expanded version of X so that its dimension agrees with Y_bin&#39;s #&#39; @return time_indicator length N_bin factor indicating the time of the data points #&#39; @return N the total number of observations of Y_i^(t) in Y #&#39; @export binningY_d1 = function(X, Y, B = 40, min_count_ratio = 0){ # now, it only works for d = 1 TT = length(Y) p = dim(X)[2] Y_range = unlist(lapply(Y, range)) minY = min(Y_range) maxY = max(Y_range) bin = seq(from=minY, to=maxY, length= B + 1) binnedY = lapply(Y, findInterval, bin, rightmost.closed = T) binnedY = lapply(binnedY, factor, levels = 1:B) count = lapply(binnedY, table) N = do.call(sum, lapply(count, sum)) mid = rep(0, B) for (i in 1:B){ mid[i] = (bin[i+1] + bin[i])/2 } # resizing Y_bin = c() Y_count = c() X_bin = c() time_indicator = c() for (t in 1:TT){ tbl = count[[t]] count_nonzero = 0 for (j in 1:B){ if (tbl[j] &gt; N*min_count_ratio/TT){ Y_bin = c(Y_bin, c(mid[j])) Y_count = c(Y_count, tbl[j]) count_nonzero = count_nonzero + 1 } } time_indicator = c(time_indicator, rep(t, count_nonzero)) new_row = t(matrix(rep(X[t,], count_nonzero), nrow = p)) X_bin = rbind(X_bin, new_row) } Y_bin = matrix(Y_bin, ncol = 1) time_indicator = factor(time_indicator) return(list(Y_bin = Y_bin, Y_count = Y_count, X_bin = X_bin, time_indicator = time_indicator, N = N )) } B = 40 K = 2 r_bar = 1e-3 lambda_theta = 0 lambda_alpha = 1e-8 min_count_ratio = 0 p = dim(X)[2] &lt;&lt;preprocessing&gt;&gt; ###&quot;preprocessing&quot;### binnedY = binningY_d1(X, Y, B) Y_bin = binnedY$Y_bin Y_count = binnedY$Y_count X_bin = binnedY$X_bin time_indicator = binnedY$time_indicator N = binnedY$N p = dim(X_bin)[2] We can see that the length of Y are reduced significantly after binning length(unlist(Y)) length(Y_bin) dim(X_bin) ## [1] 2500 ## [1] 822 ## [1] 822 2 Let’s have a look at what the binning does on our example data. par(mfrow = c(1, 2), pty = &#39;s&#39;, cex = 0.7) plot(c(0, 0), xlim = c(1, TT), ylim = range(Y_bin), col = &#39;white&#39;) for (t in 1:TT){ index = time_indicator == t points(rep(t, sum(index)), Y_bin[index], cex = Y_count[index]/mean(Y_count)) } 3.2 Initialization To initialize \\(\\theta\\) and responsibilities, we fit a Gaussian mixture of regressions model. We use flexmix to do this. Here, each point has weight, which is its resp multiplied by its Y_count. We are thresholding on the weights so that if weights is smaller than r_bar, we consider it to be zero so that we don’t take them account into our algorithm. idx indicates that whether the corresponding weight are above the threshold or not. If idx is FALSE, we don’t consider the corresponding point. ###&quot;initialize-with-flexmix&quot;### # note that it is for d = 1 # use flexmix to get initial resp, theta: flex = flexmix::flexmix(Y_bin ~ X_bin, k = K, weights = as.vector(Y_count)) # initial responsibility resp_init = flexmix::posterior(flex) # thresholding after resp * Y_count weight_init = resp_init * Y_count idx_init = weight_init &gt; r_bar # theta initialization theta0_init = list() theta_init = list() for (k in 1:K){ theta0_init[[k]] = flexmix::parameters(flex, comp = k)[1] theta_init[[k]] = matrix(flexmix::parameters(flex, comp = k)[2:(p+1)]) } usethis::use_package(&quot;flexmix&quot;) usethis::use_package(&quot;LogConcDEAD&quot;) usethis::use_package(&quot;glmnet&quot;) ## ✔ Adding &#39;flexmix&#39; to Imports field in DESCRIPTION ## • Refer to functions with `flexmix::fun()` ## ✔ Adding &#39;LogConcDEAD&#39; to Imports field in DESCRIPTION ## • Refer to functions with `LogConcDEAD::fun()` ## ✔ Adding &#39;glmnet&#39; to Imports field in DESCRIPTION ## • Refer to functions with `glmnet::fun()` Later in Mstep_g, we want to make sure that the weighted sum of residuals w.r.t. weight. This guarantees the resulting mlecld \\(g\\) of Mstep_g has a zero mean. Normally, the weighted sum of residuals w.r.t. weight is not zero, so we want to shift the \\(\\theta_0\\) so that \\(\\theta_0\\) is ‘centered’ ###&quot;X_bin-param&quot;### #&#39; @param X_bin N_bin-by-p matrix, which is expanded version of X so that its dimension agrees with Y_bin&#39;s ###&quot;Y_bin-param&quot;### #&#39; @param Y_bin N_bin-by-d matrix, indicating the center of the bin ###&quot;weight-param&quot;### #&#39; @param weight N_bin by K matrix. weight[i, k] represents weight of ith residual for kth group ###&quot;idx-param&quot;### #&#39; @param idx N_bin by K logical matrix. idx[i, k] represents whether the corresponding weight is above r_bar #&#39; Mstep_shift (d = 1) #&#39; &lt;&lt;X_bin-param&gt;&gt; &lt;&lt;Y_bin-param&gt;&gt; &lt;&lt;theta-param&gt;&gt; &lt;&lt;weight-param&gt;&gt; &lt;&lt;idx-param&gt;&gt; &lt;&lt;K-param&gt;&gt; #&#39; @return theta0 length K list, with `theta0[[k]]` being the estimate for the intercept coefficient #&#39; of the regression for kth group #&#39; @export Mstep_shift = function(X_bin, Y_bin, theta, weight, idx, K){ theta0 = list() for (k in 1:K){ idx_k = idx[,k] # except for the intercept term theta0[[k]] = t(Y_bin[idx_k,] - X_bin[idx_k,] %*% as.matrix(theta[[k]])) %*% weight[idx_k,k] / sum(weight[idx_k,k]) } return(theta0) } Using this initial responsibility, we can initialize \\(\\alpha\\). #&#39; updating alpha #&#39; &lt;&lt;X-param&gt;&gt; &lt;&lt;weight-param&gt;&gt; &lt;&lt;lambda_alpha-param&gt;&gt; #&#39; @param time_indicator length N_bin factor indicating the time of the data points #&#39; @return alpha (p+1)-by-K matrix. The coefficients for the cluster probabilities. #&#39; @export Mstep_alpha = function(X, weight, lambda_alpha, time_indicator){ lambda_max = lambda_alpha * 100 lambdas = exp(seq(from = log(lambda_max), to = log(lambda_alpha), length = 30)) weight.sum = apply(weight, 2, tapply, time_indicator, sum) fit = glmnet::glmnet(x = X, y = weight.sum, lambda = lambdas, family = &quot;multinomial&quot;, intercept = TRUE) coefs = glmnet::coef.glmnet(fit, s = lambda_alpha) alpha = as.matrix(do.call(cbind, coefs)) return(alpha) # (p+1) by K matrix } Now we make a separate function to calculate residuals, since we are going to use it pretty often. #&#39; calculating residuals for each k #&#39; &lt;&lt;X_bin-param&gt;&gt; &lt;&lt;Y_bin-param&gt;&gt; &lt;&lt;theta0-param&gt;&gt; &lt;&lt;theta-param&gt;&gt; &lt;&lt;K-param&gt;&gt; #&#39; @return length K list, with `resi0[[k]]` N_bin-by-d residual matrix #&#39; @export calc_resi = function(X_bin, Y_bin, theta0, theta, K){ resi = list() for (k in 1:K){ resi[[k]] = Y_bin - rep(theta0[[k]], dim(X_bin)[1]) - X_bin %*% matrix(theta[[k]]) } return(resi) # length K list, with `theta0[[k]]` N_bin-by-d residual matrix } Finally, based on the residuals, we calculate the mlecld g. ###&quot;resi-param&quot;### #&#39; @param resi length K list, with `resi[[k]]` N_bin-by-d residual matrix #&#39; updating `mlelcd` g&#39;s #&#39; &lt;&lt;resi-param&gt;&gt; &lt;&lt;weight-param&gt;&gt; &lt;&lt;idx-param&gt;&gt; &lt;&lt;K-param&gt;&gt; #&#39; @return g length K list, with `g[[k]]` being `mlelcd` for the kth group #&#39; @export Mstep_g = function(resi, weight, idx, K){ g = list() for (k in 1:K){ idx_k = idx[,k] suppressWarnings({g[[k]] = LogConcDEAD::mlelcd(resi[[k]][idx_k,], w = weight[idx_k,k])}) } return(g) } Thus, the initialization part is as follows: ###&quot;initialization&quot;### &lt;&lt;initialize-with-flexmix&gt;&gt; ## initial alpha alpha_init = Mstep_alpha(X, weight_init, lambda_alpha, time_indicator) ## initial theta shift theta0_init = Mstep_shift(X_bin, Y_bin, theta_init, weight_init, idx_init, K) resi_init = calc_resi(X_bin, Y_bin, theta0_init, theta_init, K) ## initial g g_init = Mstep_g(resi_init, weight_init, idx_init, K) #library(flexmix) &lt;&lt;initialization&gt;&gt; This is how initial residuals for each group look like. par(mfrow = c(1, 2), pty = &#39;s&#39;, cex = 0.7) plot(c(0, 0), xlim = c(1, TT), ylim = range(unlist(resi_init)), col = &#39;white&#39;) for (t in 1:TT){ index = time_indicator == t points(rep(t, sum(index)), resi_init[[1]][index]) } plot(c(0, 0), xlim = c(1, TT), ylim = range(unlist(resi_init)), col = &#39;white&#39;) for (t in 1:TT){ index = time_indicator == t points(rep(t, sum(index)), resi_init[[2]][index]) } Now, the points are magnified (or shrink) proportional to their weights par(mfrow = c(1, 2), pty = &#39;s&#39;, cex = 0.7) plot(c(0, 0), xlim = c(1, TT), ylim = range(unlist(resi_init)), col = &#39;white&#39;) for (t in 1:TT){ index = time_indicator == t points(rep(t, sum(index)), resi_init[[1]][index], cex = weight_init[index,1]/mean(Y_count)) } plot(c(0, 0), xlim = c(1, TT), ylim = range(unlist(resi_init)), col = &#39;white&#39;) for (t in 1:TT){ index = time_indicator == t points(rep(t, sum(index)), resi_init[[2]][index], cex = weight_init[index,2]/mean(Y_count)) } Here are the plots of initial g’s par(mfrow = c(1, 2), pty = &#39;s&#39;, cex = 0.7) plot(g_init[[1]]) plot(g_init[[2]]) The black curves in the left plot are made using initial estimates for \\(\\theta_0\\) and \\(\\theta\\). The right plot is the true plot that we saw before. par(mfrow = c(1, 2), pty = &#39;s&#39;, cex = 0.7) plot_data_and_model(X, Y, Z, theta0_init, theta_init) plot_data_and_model(X, Y, Z, theta0_true, theta_true) They are initial \\(\\theta_0\\) and \\(\\theta\\) print(theta0_init) print(theta_init) ## [[1]] ## [,1] ## [1,] -0.277917 ## ## [[2]] ## [,1] ## [1,] 4.132471 ## [[1]] ## [,1] ## [1,] 0.3759985 ## [2,] 5.1062696 ## ## [[2]] ## [,1] ## [1,] 0.6584569 ## [2,] -1.6150980 We also need some other functions to calculate \\(\\pi_{tk}(\\hat \\alpha)\\) and surrogate loglikelihood \\(Q\\) #&#39; calculating \\pi_{tk}(\\alpha) = P(Z_i^{(t)} = k| X^(t)) #&#39; &lt;&lt;X_bin-param&gt;&gt; &lt;&lt;alpha-param&gt;&gt; #&#39; @return pi_k N_bin-by-K matrix with `pi_k[i,k]` indicates P(Z_i = k| X^(t)) #&#39; @export pi_k = function(X_bin, alpha){ p = dim(X_bin)[2] tmp = exp(alpha[1,] + X_bin %*% alpha[2:(p+1),]) pi_k = tmp / rowSums(tmp) # N_bin-by-K matrix return(pi_k) } ###&quot;g-param&quot;### #&#39; @param g length K list, with `g[[k]]` being `mlelcd` for the kth group #&#39; calculating the surrogate loglikelihood Q #&#39; &lt;&lt;X_bin-param&gt;&gt; &lt;&lt;g-param&gt;&gt; &lt;&lt;resi-param&gt;&gt; &lt;&lt;theta-param&gt;&gt; &lt;&lt;alpha-param&gt;&gt; &lt;&lt;weight-param&gt;&gt; &lt;&lt;idx-param&gt;&gt; &lt;&lt;lambda_alpha-param&gt;&gt; &lt;&lt;lambda_theta-param&gt;&gt; &lt;&lt;K-param&gt;&gt; #&#39; @param N the total number of observations of Y_i^(t) in Y #&#39; @return Q the surrogate loglikelihood of the current parameters #&#39; @export # calculating the surrogate calc_surr = function(X_bin, g, resi, theta, alpha, weight, idx, lambda_alpha, lambda_theta, K, N){ P = pi_k(X_bin, alpha) # N_bin by K matrix p = dim(X_bin)[2] sum = c() for (k in 1:K){ tmp = weight[,k] * (LogConcDEAD::dlcd(resi[[k]], g[[k]], uselog = T) + log(P[,k])) tmp[!idx[,k]] = 0 sum = cbind(sum, tmp) # N_bin by K matrix } Q = mean(rowSums(sum)) - lambda_alpha * sum(abs(alpha[2:(p+1),])) / N - lambda_theta * sum(abs(unlist(theta))) / N return(Q) } It is the initial (surrogate) loglikelihood Q Q = calc_surr(X_bin, g_init, resi_init, theta_init, alpha_init, weight_init, idx_init, lambda_alpha, lambda_theta, K, N) print(Q) ## [1] -7.501209 3.3 E-step Given an estimate of \\((\\alpha,\\theta,g)\\), the E-step computes for each \\(Y_{i}^{(t)}\\) how “responsible” each cluster is for it. In particular, the responsibility vector \\((\\hat r_{ti1},\\ldots,\\hat r_{tiK})\\) is a probability vector. It is computed using Bayes rule: \\[ \\hat r_{tik} = \\hat P(Z_i^{(t)} = k |Y_i^{(t)} , X^{(t)} ; \\hat \\alpha, \\hat \\theta, \\hat g) = \\frac{exp(\\hat g_k(Y_i^{(t)} -\\hat \\theta_{k0} -\\hat \\theta_k^T X^{(t)} )) \\pi_{tk}(\\hat \\alpha)} {\\sum_{l=1}^K exp(\\hat g_l(Y_i^{(t)} -\\hat \\theta_{l0} -\\hat \\theta_l^T X^{(t)} )) \\pi_{tl}(\\hat \\alpha) } \\] ###&quot;Y_count-param&quot;### #&#39; @param Y_count N_bin-vector indicating how many points belong to each bin #&#39; Updating responsibility #&#39; &lt;&lt;X_bin-param&gt;&gt; &lt;&lt;Y_count-param&gt;&gt; &lt;&lt;resi-param&gt;&gt; &lt;&lt;alpha-param&gt;&gt; &lt;&lt;g-param&gt;&gt; &lt;&lt;K-param&gt;&gt; &lt;&lt;r_bar-param&gt;&gt; #&#39; @return resp N_bin by K matrix. `resp[i, k]` being the current estimate for P(Z_i = k|X, Y) #&#39; @return weight N_bin by K matrix. weight[i, k] represents weight of ith residual for kth group #&#39; @return idx N_bin by K logical matrix. idx[i, k] represents whether the corresponding weight is above r_bar #&#39; @export E_step = function(X_bin, Y_count, resi, alpha, g, K, r_bar){ P = pi_k(X_bin, alpha) sum = c() for (k in 1:K){ tmp = LogConcDEAD::dlcd(resi[[k]], g[[k]]) * P[,k] sum = cbind(sum, tmp) } resp = sum/rowSums(sum) if (sum(is.nan(resp)) &gt; 0) { print(&quot;There are some NaN&#39;s in the new responsibilties&quot;) } weight = resp * Y_count idx = weight &gt; r_bar return(list(resp = resp, weight = weight, idx = idx)) } ###&quot;E-step&quot;### ## E-step: update responsibilities Estep = E_step(X_bin, Y_count, resi_old, alpha_old, g_old, K, r_bar) resp_new = Estep$resp weight_new = Estep$weight idx_new = Estep$idx idx_old = idx_init resi_old = resi_init alpha_old = alpha_init g_old = g_init &lt;&lt;E-step&gt;&gt; Let’s have a look at the responsibilities that we get after E-step. Here are some from the 1st time point: resp_new[1:6, ] ## tmp tmp ## [1,] 0.9249669 0.07503310 ## [2,] 0.9692050 0.03079502 ## [3,] 0.9526349 0.04736511 ## [4,] 0.9103402 0.08965976 ## [5,] 0.8165541 0.18344594 ## [6,] 0.7003569 0.29964305 3.4 M-step In the M-step, we update the estimates of \\((\\alpha,\\theta,g)\\): ###&quot;M-step&quot;### ## M-step: update estimates of (alpha,theta,g) ### Mstep_alpha alpha_new = Mstep_alpha(X, weight_new, lambda_alpha, time_indicator) Q_every = append(Q_every, calc_surr(X_bin, g_old, resi_old, theta_old, alpha_new, weight_new, idx_new, lambda_alpha, lambda_theta, K, N)) ### Mstep_theta M_theta = Mstep_theta(X_bin, X, Y_bin, g_old, weight_new, idx_old, theta0_old, theta_old, lambda_theta, K, time_indicator) theta0_new = M_theta$theta0 theta_new = M_theta$theta ### Mstep_shift theta0_new = Mstep_shift(X_bin, Y_bin, theta_new, weight_new, idx_new, K) resi_new = calc_resi(X_bin, Y_bin, theta0_new, theta_new, K) Q_every = append(Q_every, calc_surr(X_bin, g_old, resi_new, theta_new, alpha_new, weight_new, idx_new, lambda_alpha, lambda_theta, K, N)) ### Mstep_g g_new = Mstep_g(resi_new, weight_new, idx_new, K) Q_every = append(Q_every, calc_surr(X_bin, g_new, resi_new, theta_new, alpha_new, weight_new, idx_new, lambda_alpha, lambda_theta, K, N)) ### loglikelihood Q = append(Q, calc_surr(X_bin, g_new, resi_new, theta_new, alpha_new, weight_new, idx_new, lambda_alpha, lambda_theta, K, N)) We have already seen the Mstep_alpha(), Mstep_shift(), and Mstep_g(). We only need to defineMstep_theta()` here. #&#39; Updating theta #&#39; &lt;&lt;X_bin-param&gt;&gt; &lt;&lt;X-param&gt;&gt; &lt;&lt;Y_bin-param&gt;&gt; &lt;&lt;g-param&gt;&gt; &lt;&lt;weight-param&gt;&gt; #&#39; @param idx_old N_bin by K logical matrix, but used in the previous iteration. #&#39; idx[i, k] represents whether the corresponding previous weight is above r_bar &lt;&lt;theta0-param&gt;&gt; &lt;&lt;theta-param&gt;&gt; &lt;&lt;lambda_theta-param&gt;&gt; &lt;&lt;K-param&gt;&gt; #&#39; @param time_indicator length N_bin factor indicating the time of the data points #&#39; @return theta0 length K list, with `theta0[[k]]` being the estimate for the intercept coefficient #&#39; of the regression for kth group #&#39; @return theta length K list, with `theta[[k]]` being the p-by-1 vector of estimate for coefficients #&#39; of the regression for kth group #&#39; @export Mstep_theta = function(X_bin, X, Y_bin, g, weight, idx_old, theta0, theta, lambda_theta, K, time_indicator){ theta0_new = list() theta_new = list() for (k in 1:K){ tmp = LP_d1(X_bin, X, Y_bin, g[[k]], weight[,k], idx_old[,k], theta0[[k]], theta[[k]], lambda_theta, time_indicator) theta0_new[[k]] = tmp$theta0_k theta_new[[k]] = tmp$theta_k } return(list(&#39;theta0&#39; = theta0_new , &#39;theta&#39; = theta_new )) } We need an internal function which calculate theta using LP for each k #&#39; Updating theta for each k #&#39; &lt;&lt;X_bin-param&gt;&gt; &lt;&lt;X-param&gt;&gt; &lt;&lt;Y_bin-param&gt;&gt; #&#39; @param g_k `mlelcd` for the kth group #&#39; @param weight_k N_bin vector of weight for kth group #&#39; @param idx_old_k N_bin logical vector, but used in the previous iteration. #&#39; idx_old_k[i] represents whether the corresponding previous weight for kth group is above r_bar #&#39; @param theta0_k an intercept coefficient for the regression for the kth group #&#39; @param theta_k a p-vector of coefficients for the regression for the kth group &lt;&lt;lambda_theta-param&gt;&gt; #&#39; @param time_indicator length N_bin factor indicating the time of the data points #&#39; @return theta0_k estimate for the intercept coefficient of the regression for kth group #&#39; @return theta_k p-vector of estimates for coefficients of the regression for kth group #&#39; @export LP_d1 = function(X_bin, X, Y_bin, g_k, weight_k, idx_old_k, theta0_k, theta_k, lambda_theta, time_indicator){ weight = weight_k[idx_old_k] Y_idx = Y_bin[idx_old_k] X_idx = X_bin[idx_old_k,] p = dim(X_idx)[2] n = dim(X_idx)[1] # the number of points in C_n J = length(g_k$beta) # the number of affine functions resi = Y_idx - rep(theta0_k, n) - X_idx %*% as.matrix(theta_k) L = min(resi) U = max(resi) const_mat = matrix(0, nrow = J*n + 2*TT, ncol = 2*(n+p+1)) const_vec = rep(0, J*n + 2*TT) # epigraph part for (i in 1:n) { for (j in 1:J) { const_mat[(i-1)*J + j, i] = 1 const_mat[(i-1)*J + j, n + i] = -1 } const_mat[((i-1)*J + 1):((i-1)*J + J), (2*n+1)] = g_k$b const_mat[((i-1)*J + 1):((i-1)*J + J), (2*n+2)] = -g_k$b const_mat[((i-1)*J + 1):((i-1)*J + J), (2*n+3):(2*n+p+2)] = g_k$b %*% X_idx[i,] const_mat[((i-1)*J + 1):((i-1)*J + J), (2*n+p+3):(2*(n+p+1))] = - const_mat[((i-1)*J + 1):((i-1)*J + J), (2*n+3):(2*n+p+2)] const_vec[((i-1)*J + 1):((i-1)*J + J)] = Y_idx[i] * g_k$b - g_k$beta } # feasibility part const_mat[(J*n+1):(J*n+TT), 2*n+1] = rep(1, TT) const_mat[(J*n+1):(J*n+TT), 2*n+2] = -rep(1, TT) const_mat[(J*n+1):(J*n+TT), 2*(n+1)+ (1:p)] = X const_mat[(J*n+1):(J*n+TT), 2*(n+1)+p+ (1:p)] = -X const_mat[(J*n+TT+1):(J*n+TT+TT), 2*n+(1:(2*(p+1)))] = -const_mat[(J*n+1):(J*n+TT), 2*n+(1:(2*(p+1)))] for (t in 1:TT){ const_vec[J*n + t] = min(Y_bin[time_indicator == t &amp; idx_old_k])- L const_vec[J*n+TT + t] = U - max(Y_bin[time_indicator == t &amp; idx_old_k]) } obj_coef = c(weight, -weight, 0, 0, rep(-lambda_theta, 2*p)) const_dir = rep(&quot;&lt;=&quot;, J*n + 2*TT) # solving LP lp_res = Rsymphony::Rsymphony_solve_LP(obj = obj_coef, mat = const_mat, dir = const_dir, rhs = const_vec, max = T) theta0_k = lp_res$solution[2*n+1] - lp_res$solution[2*n+2] theta_k = as.matrix(lp_res$solution[(2*n+3):((2*n+p+2))] - lp_res$solution[(2*n+p+3):(2*(n+p+1))]) return(list(theta0_k = theta0_k, theta_k = theta_k)) #theta } 3.5 Trying out the method #&#39; Mixture of log-concave regression #&#39; &lt;&lt;X-param&gt;&gt; &lt;&lt;Y-param&gt;&gt; &lt;&lt;K-param&gt;&gt; &lt;&lt;B-param&gt;&gt; #&#39; @param min_count_ratio &lt;&lt;r_bar-param&gt;&gt; &lt;&lt;lambda_alpha-param&gt;&gt; &lt;&lt;lambda_theta-param&gt;&gt; #&#39; @param max_iter number of maximum iterations of EM to perform #&#39; @param iter_eta threshold for the iteration. If the increment of loglikelihood is smaller than iter_eta, #&#39; we terminate the iterations. #&#39; @return X a T-by-p matrix of covariates, where `X[[t]]` being the p-vector of independent variable at time t #&#39; @return Y length T list with `Y[[t]]` being a n_t-by-d matrix #&#39; @return Y_count N_bin-vector indicating how many points belong to each bin #&#39; @return N the total number of observations of Y_i^(t) in Y #&#39; @return resp_init N_bin by K matrix. `resp_init[i, k]` being the initial estimate for P(Z_i = k|X, Y) #&#39; @return weight_init N_bin by K matrix. weight_init[i, k] represents initial weight of ith residual for kth group #&#39; @return idx_init N_bin by K logical matrix. idx_init[i, k] represents whether the corresponding initial weight is above r_bar #&#39; @return theta0_init length K list, with `theta0_init[[k]]` being the initial estimate for the intercept coefficient of the regression for kth group #&#39; @return theta_init length K list, with `theta_init[[k]]` being the p-by-1 vector. #&#39; Initial estimates for coefficients of the regression for kth group #&#39; @return alpha_init (p+1)-by-K matrix. The coefficients for the initial cluster probabilities. #&#39; @return resi_init length K list with `resi_init[[k]]` N_bin-by-d initial residual matrix #&#39; @return g_init length K list, with `g_init[[k]]` being the initial `mlelcd` for the kth group #&#39; @return resp N_bin by K matrix. `resp[i, k]` being the final estimate for P(Z_i = k|X, Y) #&#39; @return weight N_bin by K matrix. weight[i, k] represents initial weight of ith residual for kth group #&#39; @return idx N_bin by K logical matrix. idx[i, k] represents whether the corresponding final weight is above r_bar #&#39; @return theta0 length K list, with `theta0[[k]]` being the final estimate for the intercept coefficient of the regression for kth group #&#39; @return theta length K list, with `theta[[k]]` being the p-by-1 vector. #&#39; Final estimates for coefficients of the regression for kth group #&#39; @return alpha (p+1)-by-K matrix. The coefficients for the cluster probabilities. #&#39; @return resi length K list with `resi[[k]]` N_bin-by-d final residual matrix #&#39; @return g length K list, with `g[[k]]` being the final `mlelcd` for the kth group #&#39; @return Q a vector of the surrogate loglikelihoods of the parameters, stored at each iteration #&#39; @return Q_every a vector of the surrogate loglikelihoods of the parameters, stored at each step #&#39; @export mixLcdReg &lt;- function(X, Y, K, B = 40, min_count_ratio = 0, r_bar, lambda_alpha, lambda_theta, max_iter = 100, iter_eta = 1e-6) { # preprocessing &lt;&lt;preprocessing&gt;&gt; # initialization &lt;&lt;initialization&gt;&gt; ## initial Q Q = calc_surr(X_bin, g_init, resi_init, theta_init, alpha_init, weight_init, idx_init, lambda_alpha, lambda_theta, K, N) Q_every = Q idx_old = idx_init resi_old = resi_init alpha_old = alpha_init theta0_old = theta0_init theta_old = theta_init g_old = g_init # iteration for (i in seq(max_iter)) { ## E-step &lt;&lt;E-step&gt;&gt; Q_every = append(Q_every, calc_surr(X_bin, g_old, resi_old, theta_old, alpha_old, weight_new, idx_new, lambda_alpha, lambda_theta, K, N)) ## M-step &lt;&lt;M-step&gt;&gt; ## termination criteria if (Q[i+1]-Q[i] &lt;= iter_eta | i==max_iter){ break; } else { idx_old = idx_new resi_old = resi_new alpha_old = alpha_new theta0_old = theta0_new theta_old = theta_new g_old = g_new } } print(i) return(list(X = X, X_bin = X_bin, Y = Y, Y_bin = Y_bin, Y_count = Y_count, N = N, resp_init = resp_init, weight_init = weight_init, idx_init = idx_init, theta0_init = theta0_init, theta_init = theta_init, alpha_init = alpha_init, resi_init = resi_init, g_init = g_init, resp = resp_new, weight = weight_new, idx = idx_new, theta0 = theta0_new, theta = theta_new, alpha = alpha_new, resi = resi_new, g = g_new, Q = Q, Q_every = Q_every)) } Let’s try out our method mix1 = mixLcdReg(X = X, Y = Y, K = 2, B = 40, min_count_ratio = 0.01, r_bar = 1e-3, lambda_alpha = 1e-8, lambda_theta = 0) ## [1] 9 Here’s Loglikelihood plot par(mfrow = c(1, 2), pty = &#39;s&#39;, cex = 0.8) plot(0:(length(mix1$Q)-1), mix1$Q, type =&#39;b&#39;, main = &#39;The mixture of loglikelihood&#39;) print(mix1$Q) ## [1] -7.343648 -6.749344 -6.596104 -6.476042 -6.384549 -6.341304 -6.332476 ## [8] -6.331027 -6.330744 -6.330949 Here’s the true \\(\\theta_0\\), initial estimate \\(\\hat \\theta_0\\), and the final estimate \\(\\hat \\theta_0\\) print(theta0_true) print(mix1$theta0_init) print(mix1$theta0) ## [[1]] ## [1] 0 ## ## [[2]] ## [1] 5 ## [[1]] ## [,1] ## [1,] 1.332908 ## ## [[2]] ## [,1] ## [1,] 4.575933 ## [[1]] ## [,1] ## [1,] 1.194819 ## ## [[2]] ## [,1] ## [1,] 4.7422 Here’s the true \\(\\theta\\), initial estimate \\(\\hat \\theta\\), and the final estimate \\(\\hat \\theta\\) print(theta_true) print(mix1$theta_init) print(mix1$theta) ## [[1]] ## [1] 0 0 ## ## [[2]] ## [1] 0.5 0.0 ## [[1]] ## [,1] ## [1,] 0.3786654 ## [2,] 1.7550216 ## ## [[2]] ## [,1] ## [1,] 0.54648060 ## [2,] -0.02191341 ## [[1]] ## [,1] ## [1,] 0 ## [2,] 0 ## ## [[2]] ## [,1] ## [1,] 0.5388235 ## [2,] 0.0000000 Here’s the true \\(\\alpha\\), initial estimate \\(\\hat \\alpha\\), and the final estimate \\(\\hat \\alpha\\) print(alpha_true) print(mix1$alpha_init) print(mix1$alpha) ## [,1] [,2] ## [1,] 0 0 ## [2,] 0 0 ## [3,] 0 1 ## 1 1 ## (Intercept) 0.433820581 -0.433820581 ## X1 0.007470917 -0.007470917 ## X2 -0.241154514 0.241154514 ## 1 1 ## (Intercept) 0.23486024 -0.23486024 ## X1 -0.03476891 0.03476891 ## X2 -0.48760016 0.48760016 Here are the plots of final g’s. The red curves are the true g. minI = min(unlist(mix1$resi)) maxI = max(unlist(mix1$resi)) grid0 = seq(minI, maxI, length = 200) par(mfrow = c(1, 2), pty = &#39;s&#39;, cex = 0.7) plot(mix1$g[[1]]) lines(grid0, dnorm(grid0), lwd = 1, col = &#39;red&#39;) plot(mix1$g[[2]], xlab = &#39;residuals&#39;) lines(grid0, exp(-grid0-1), lwd = 1, col = &#39;red&#39;) The black curves in the left plot are made using final estimates for \\(\\theta_0\\) and \\(\\theta\\). The right plot is the true plot that we saw before. par(mfrow = c(1, 2), pty = &#39;s&#39;, cex = 0.7) plot_data_and_model(X, Y, Z, mix1$theta0, mix1$theta) plot_data_and_model(X, Y, Z, theta0_true, theta_true) "],["conclude.html", "4 Conclusion", " 4 Conclusion When you are done defining the package, it remains to convert the Roxygen to documentation. litr::document() # &lt;-- use instead of devtools::document() ## ℹ Updating mixLcdReg documentation ## ℹ Loading mixLcdReg ## Writing &#39;NAMESPACE&#39; ## Writing &#39;mixLcdReg-package.Rd&#39; ## Writing &#39;mixLcdReg.Rd&#39; ## Writing &#39;plot_data.Rd&#39; ## Writing &#39;plot_data_and_model.Rd&#39; You can also add some extra things to your package here if you like, such as a README, some vignettes, a pkgdown site, etc. See here for an example of how to do this with litr. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
